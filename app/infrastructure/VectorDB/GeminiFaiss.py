import os
import time
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from langchain_core.documents import Document
from typing import List
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
load_dotenv()

class GeminiFaiss():
    def __init__(self):
        self.persist_path = os.getenv("PERSIST_PATH")
        self.model = os.getenv("EMBEDDING_MODEL", "models/gemini-embedding-001")
        # TÄƒng timeout vÃ  retry cho Gemini API
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=self.model,
            google_api_key=os.getenv("GOOGLE_API_KEY"),
            # ThÃªm config Ä‘á»ƒ tá»‘i Æ°u
            task_type="RETRIEVAL_DOCUMENT",
            # CÃ³ thá»ƒ thÃªm rate limiting náº¿u cáº§n
        )
        vector_store = None

        # file FAISS index
        index_file = os.path.join(self.persist_path, "index.faiss")

        if os.path.exists(index_file):
            try:
                vector_store = FAISS.load_local(
                    self.persist_path,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                ) 
                print(f"âœ… ÄÃ£ load vectorstore tá»«: {self.persist_path}")
            except Exception as e:
                print(f"âŒ Lá»—i khi load vectorstore: {e}")
                vector_store = None
        else:
            print(f"âš ï¸ ChÆ°a cÃ³ vectorstore táº¡i: {self.persist_path}. Sáº½ táº¡o má»›i sau.")

        super().__init__(vector_store)

    def save_vectorstore(self):
        print(f"ğŸ’¾ Äang lÆ°u vectorstore vÃ o: {self.persist_path}")
        if self.vector_store:
            self.vector_store.save_local(self.persist_path)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u vectorstore vÃ o: {self.persist_path}")
            return True
        return False
    
    def add_documents_batch(self, documents: List[Document], batch_size: int = 3, delay: float = 5.0):
        """ThÃªm documents theo batch vá»›i exponential backoff Ä‘á»ƒ trÃ¡nh rate limit"""
        if not documents:
            print("â— KhÃ´ng cÃ³ document nÃ o Ä‘á»ƒ thÃªm.")
            return

        print(f"ğŸš€ Báº¯t Ä‘áº§u embedding {len(documents)} documents vá»›i batch_size={batch_size}")
        
        # Chia documents thÃ nh batches nhá» hÆ¡n
        batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]
        
        all_embeddings = []
        total_time = 0
        
        for i, batch in enumerate(batches):
            start_time = time.time()
            print(f"ğŸ“¦ Xá»­ lÃ½ batch {i+1}/{len(batches)} ({len(batch)} docs)...")
            
            retry_count = 0
            max_retries = 5
            base_delay = delay
            
            while retry_count <= max_retries:
                try:
                    # Embed tá»«ng document má»™t Ä‘á»ƒ trÃ¡nh rate limit
                    batch_embeddings = []
                    for j, doc in enumerate(batch):
                        print(f"  ğŸ“„ Embedding doc {j+1}/{len(batch)} trong batch {i+1}...")
                        doc_embedding = self.embeddings.embed_documents([doc.page_content])
                        batch_embeddings.extend(doc_embedding)
                        
                        # Delay nhá» giá»¯a cÃ¡c document
                        if j < len(batch) - 1:
                            time.sleep(0.5)
                    
                    all_embeddings.extend(batch_embeddings)
                    batch_time = time.time() - start_time
                    total_time += batch_time
                    print(f"âœ… HoÃ n thÃ nh batch {i+1} trong {batch_time:.2f}s")
                    break
                    
                except Exception as e:
                    retry_count += 1
                    if "429" in str(e) or "exhausted" in str(e):
                        # Exponential backoff
                        wait_time = base_delay * (2 ** retry_count)
                        print(f"ğŸš« Rate limit hit! Retry {retry_count}/{max_retries} sau {wait_time:.1f}s...")
                        time.sleep(wait_time)
                    else:
                        print(f"âŒ Lá»—i khÃ¡c: {e}")
                        raise
            
            if retry_count > max_retries:
                print(f"âŒ ÄÃ£ retry {max_retries} láº§n, bá» qua batch nÃ y")
                continue
                
            # Delay dÃ i giá»¯a cÃ¡c batch
            if i < len(batches) - 1:
                print(f"â³ Chá» {delay}s trÆ°á»›c batch tiáº¿p theo...")
                time.sleep(delay)

        print(f"â° Tá»•ng thá»i gian embedding: {total_time:.2f}s")
        
        # Táº¡o hoáº·c cáº­p nháº­t vectorstore
        if self.vector_store:
            self.vector_store.add_documents(documents)
            print(f"â• ÄÃ£ thÃªm {len(documents)} documents vÃ o vectorstore.")
        else:
            self.vector_store = FAISS.from_documents(documents, self.embeddings)
            print(f"ğŸ†• ÄÃ£ táº¡o vectorstore má»›i tá»« {len(documents)} documents.")

        # LÆ°u vectorstore
        os.makedirs(self.persist_path, exist_ok=True)
        self.vector_store.save_local(self.persist_path)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u vectorstore táº¡i: {self.persist_path}")

    def add_documents(self, documents: List[Document]):
        """Wrapper cho add_documents_batch vá»›i config máº·c Ä‘á»‹nh"""
        self.add_documents_batch(documents, batch_size=5, delay=1.5)

    def add_documents_optimized(self, documents: List[Document], chunk_size: int = 512):
        """Version tá»‘i Æ°u vá»›i text chunking náº¿u documents quÃ¡ dÃ i"""
        if not documents:
            print("â— KhÃ´ng cÃ³ document nÃ o Ä‘á»ƒ thÃªm.")
            return

        # Chunk documents dÃ i thÃ nh pieces nhá» hÆ¡n
        chunked_docs = []
        for doc in documents:
            if len(doc.page_content) > chunk_size:
                # Chia document dÃ i thÃ nh chunks
                content = doc.page_content
                for i in range(0, len(content), chunk_size):
                    chunk_content = content[i:i + chunk_size]
                    # Táº¡o metadata má»›i cho chunk
                    chunk_metadata = doc.metadata.copy()
                    chunk_metadata['chunk_id'] = f"{doc.metadata.get('row_index', 0)}_chunk_{i//chunk_size}"
                    chunk_metadata['is_chunked'] = True
                    
                    chunked_docs.append(Document(
                        page_content=chunk_content,
                        metadata=chunk_metadata
                    ))
            else:
                chunked_docs.append(doc)
        
        print(f"ğŸ”„ ÄÃ£ chunk {len(documents)} docs thÃ nh {len(chunked_docs)} pieces")
        
        # Sá»­ dá»¥ng batching
        self.add_documents_batch(chunked_docs, batch_size=8, delay=1.0)