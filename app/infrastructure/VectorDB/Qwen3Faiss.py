import os
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from core.interface.IQwen3Faiss import IQwen3Faiss
from typing import List
from dotenv import load_dotenv
import time

load_dotenv()

class Qwen3Faiss(IQwen3Faiss):
    def __init__(self):
        self.persist_path = os.getenv("PERSIST_PATH")
        
        # S·ª≠ d·ª•ng Qwen3-Embedding-0.6B - SOTA model
        print("üîÑ ƒêang load Qwen3-Embedding-0.6B model...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="Qwen/Qwen3-Embedding-0.6B",
            model_kwargs={
                'device': 'cpu',  # D√πng 'cuda' n·∫øu c√≥ GPU
                'trust_remote_code': True,  # C·∫ßn thi·∫øt cho Qwen models
            },
            encode_kwargs={
                'normalize_embeddings': True,
                'batch_size': 32,  # TƒÉng batch size v√¨ model m·∫°nh
                # C√≥ th·ªÉ th√™m instruction ƒë·ªÉ tƒÉng 1-5% performance
                'instruction': 'Represent this document for retrieval: '
            }
        )
        print("‚úÖ ƒê√£ load Qwen3-Embedding-0.6B model!")
        
        vector_store = None
        index_file = os.path.join(self.persist_path, "index.faiss")

        if os.path.exists(index_file):
            try:
                vector_store = FAISS.load_local(
                    self.persist_path,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                ) 
                print(f"‚úÖ ƒê√£ load vectorstore t·ª´: {self.persist_path}")
            except Exception as e:
                print(f"‚ùå L·ªói khi load vectorstore: {e}")
                vector_store = None
        else:
            print(f"‚ö†Ô∏è Ch∆∞a c√≥ vectorstore t·∫°i: {self.persist_path}. S·∫Ω t·∫°o m·ªõi sau.")

        self.vector_store = vector_store

    def save_vectorstore(self) -> bool:
        """L∆∞u vectorstore v√†o persist_path"""
        print(f"üíæ ƒêang l∆∞u vectorstore v√†o: {self.persist_path}")
        try:
            if self.vector_store:
                os.makedirs(self.persist_path, exist_ok=True)
                self.vector_store.save_local(self.persist_path)
                print(f"üíæ ƒê√£ l∆∞u vectorstore v√†o: {self.persist_path}")
                return True
            else:
                print("‚ùå Kh√¥ng c√≥ vectorstore ƒë·ªÉ l∆∞u!")
                return False
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u vectorstore: {e}")
            return False
    
    def add_documents(self, documents: List[Document]) -> bool:
        """Th√™m documents v·ªõi Qwen3 - NHANH v√† CH·∫§T L∆Ø·ª¢NG CAO"""
        if not documents:
            print("‚ùó Kh√¥ng c√≥ document n√†o ƒë·ªÉ th√™m.")
            return False

        try:
            print(f"üöÄ B·∫Øt ƒë·∫ßu embedding {len(documents)} documents v·ªõi Qwen3-0.6B...")
            start_time = time.time()

            if self.vector_store:
                self.vector_store.add_documents(documents)
                print(f"‚ûï ƒê√£ th√™m {len(documents)} documents v√†o vectorstore.")
            else:
                self.vector_store = FAISS.from_documents(documents, self.embeddings)
                print(f"üÜï ƒê√£ t·∫°o vectorstore m·ªõi t·ª´ {len(documents)} documents.")

            # T·ª± ƒë·ªông l∆∞u sau khi th√™m
            save_success = self.save_vectorstore()
            
            total_time = time.time() - start_time
            print(f"‚è∞ Ho√†n th√†nh trong {total_time:.2f}s (trung b√¨nh {total_time/len(documents):.2f}s/doc)")
            
            return save_success
            
        except Exception as e:
            print(f"‚ùå L·ªói khi th√™m documents: {e}")
            return False

    def add_documents_with_custom_instruction(self, documents: List[Document], instruction: str) -> bool:
        """Th√™m documents v·ªõi custom instruction ƒë·ªÉ tƒÉng performance"""
        if not documents:
            print("‚ùó Kh√¥ng c√≥ document n√†o ƒë·ªÉ th√™m.")
            return False

        if not instruction:
            print("‚ö†Ô∏è Kh√¥ng c√≥ instruction, s·ª≠ d·ª•ng add_documents th√¥ng th∆∞·ªùng.")
            return self.add_documents(documents)

        try:
            print(f"üéØ S·ª≠ d·ª•ng custom instruction: '{instruction[:50]}...'")
            
            # T·∫°o embedding instance m·ªõi v·ªõi instruction kh√°c
            custom_embeddings = HuggingFaceEmbeddings(
                model_name="Qwen/Qwen3-Embedding-0.6B",
                model_kwargs={
                    'device': 'cpu',
                    'trust_remote_code': True,
                },
                encode_kwargs={
                    'normalize_embeddings': True,
                    'batch_size': 32,
                    'instruction': instruction
                }
            )
            
            # Backup embedding hi·ªán t·∫°i
            original_embeddings = self.embeddings
            self.embeddings = custom_embeddings
            
            # Th√™m documents
            success = self.add_documents(documents)
            
            # Restore embedding
            self.embeddings = original_embeddings
            
            return success
            
        except Exception as e:
            print(f"‚ùå L·ªói khi th√™m documents v·ªõi custom instruction: {e}")
            # Restore embedding trong tr∆∞·ªùng h·ª£p l·ªói
            if 'original_embeddings' in locals():
                self.embeddings = original_embeddings
            return False

    def add_documents_optimized(self, documents: List[Document], chunk_size: int = 1000) -> bool:
        """Version t·ªëi ∆∞u v·ªõi chunking ph√π h·ª£p cho Qwen3"""
        if not documents:
            print("‚ùó Kh√¥ng c√≥ document n√†o ƒë·ªÉ th√™m.")
            return False

        try:
            print(f"üîß T·ªëi ∆∞u h√≥a documents v·ªõi chunk_size={chunk_size}")
            
            # Chunk v·ªõi size l·ªõn h∆°n v√¨ Qwen3 x·ª≠ l√Ω context d√†i t·ªët h∆°n
            chunked_docs = []
            for doc in documents:
                if len(doc.page_content) > chunk_size:
                    content = doc.page_content
                    for i in range(0, len(content), chunk_size):
                        chunk_content = content[i:i + chunk_size]
                        chunk_metadata = doc.metadata.copy()
                        chunk_metadata['chunk_id'] = f"{doc.metadata.get('row_index', 0)}_chunk_{i//chunk_size}"
                        chunk_metadata['is_chunked'] = True
                        
                        chunked_docs.append(Document(
                            page_content=chunk_content,
                            metadata=chunk_metadata
                        ))
                else:
                    chunked_docs.append(doc)
            
            if len(chunked_docs) != len(documents):
                print(f"üîÑ ƒê√£ chunk {len(documents)} docs th√†nh {len(chunked_docs)} pieces")
            
            # S·ª≠ d·ª•ng instruction ph√π h·ª£p cho technical documents
            return self.add_documents_with_custom_instruction(
                chunked_docs, 
                instruction="Represent this technical document for semantic search and retrieval: "
            )
            
        except Exception as e:
            print(f"‚ùå L·ªói khi t·ªëi ∆∞u h√≥a documents: {e}")
            return False
    
    def similarity_search(self, query: str, k: int = 5, score_threshold: float = 0.0):
        """Bonus method: T√¨m ki·∫øm documents t∆∞∆°ng t·ª±"""
        if not self.vector_store:
            print("‚ùå Ch∆∞a c√≥ vectorstore ƒë·ªÉ search!")
            return []
        
        try:
            results = self.vector_store.similarity_search_with_score(query, k=k)
            # Filter theo score threshold n·∫øu c·∫ßn
            filtered_results = [(doc, score) for doc, score in results if score >= score_threshold]
            return filtered_results
        except Exception as e:
            print(f"‚ùå L·ªói khi search: {e}")
            return []
    
    def get_vectorstore_info(self) -> dict:
        """Bonus method: L·∫•y th√¥ng tin v·ªÅ vectorstore"""
        if not self.vector_store:
            return {"status": "empty", "count": 0}
        
        try:
            # Th·ª≠ l·∫•y s·ªë l∆∞·ª£ng documents (c√≥ th·ªÉ kh√¥ng work v·ªõi t·∫•t c·∫£ FAISS versions)
            info = {
                "status": "loaded",
                "persist_path": self.persist_path,
                "embedding_model": "Qwen/Qwen3-Embedding-0.6B"
            }
            
            # Th·ª≠ l·∫•y s·ªë vectors n·∫øu c√≥ th·ªÉ
            if hasattr(self.vector_store, 'index') and hasattr(self.vector_store.index, 'ntotal'):
                info["vector_count"] = self.vector_store.index.ntotal
            
            return info
        except Exception as e:
            return {"status": "error", "error": str(e)}


# Alternative: S·ª≠ d·ª•ng tr·ª±c ti·∫øp sentence-transformers cho flexibility cao h∆°n
class DirectQwen3EmbeddingService:
    def __init__(self):
        from sentence_transformers import SentenceTransformer
        print("üîÑ ƒêang load Qwen3-Embedding-0.6B v·ªõi SentenceTransformers...")
        
        self.model = SentenceTransformer(
            "Qwen/Qwen3-Embedding-0.6B",
            trust_remote_code=True,
            device='cpu'  # ho·∫∑c 'cuda'
        )
        print("‚úÖ ƒê√£ load xong!")
    
    def embed_documents(self, texts: List[str], instruction: str = None) -> List[List[float]]:
        """Embed documents v·ªõi optional instruction"""
        if instruction:
            texts = [f"{instruction}{text}" for text in texts]
        
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=True,
            batch_size=32,
            show_progress_bar=True
        )
        return embeddings.tolist()
    
    def embed_query(self, query: str, instruction: str = "Represent this query for retrieving relevant documents: ") -> List[float]:
        """Embed query v·ªõi instruction ph√π h·ª£p"""
        if instruction:
            query = f"{instruction}{query}"
        
        embedding = self.model.encode(
            query,
            normalize_embeddings=True
        )
        return embedding.tolist()